= Processing long iterations in publishing domain  =

process_in_batches() function was designed to allow us to perform
read-only python domain iterations over a large result set keeping
memory usage acceptable.

It uses `LoopTunner` to process the given 'task' against batches of
the also given 'input' result_set. After each batch is finished it
cleans up the ORM caches, eliminating cached references that won't be
used anymore.

It's important to note that this mechanism is clearly only suitable
for read-only iterations, any modification done in the database
records by 'task' will be ignored.

    >>> from canonical.archivepublisher.utils import process_in_batches

    >>> class DebugStdOutLogger:
    ...     def debug(self, text):
    ...         print 'DEBUG:', text
    >>> logger = DebugStdOutLogger()

The expected 'input' argument is a `SelectResult`. We will use a
result set containing all PUBLISHED `BinaryPackagePublishingHistory`
records.

    >>> from canonical.launchpad.database.publishing import (
    ...     BinaryPackagePublishingHistory)
    >>> from canonical.launchpad.interfaces.publishing import (
    ...     PackagePublishingStatus)

    >>> input = BinaryPackagePublishingHistory.selectBy(
    ...     status=PackagePublishingStatus.PUBLISHED, orderBy=['id'])

We have 20 records in our 'input' result set.

    >>> print input.count()
    20

As 'task' we will define a callable that simply extract the binary
publication name.

    >>> def simple_task(binary_publication):
    ...     ignore = binary_publication.binarypackagerelease.name

That's all we need, calling process_in_batches() with the specific
'input', 'task' and a 'logger' instance will run as expected.

It uses the given 'logger' to print information (in debug level) about
the batches being processed and the amount of resident memory used by
the process calling it.

By default is starts with 10000 records batches and the LoopTuner will
adjust it according the given 'goal_seconds' when necessary.

    >>> process_in_batches(input, simple_task, logger)
    DEBUG: Batch [0..10000) [... MiB]


== Coping with changes in the given input ==

The current callsites use process_in_batches() mainly to dump
information from all PUBLISHED records in a given context.

Since they are the same parts that publish new packages, we can
guarantee that the result set size will never increase.

On the other hand, it may decrease while the batches get
processed. For instance, when a package gets deleted or obsoleted,
which are always performed by an user, so we can't predict when such
thing will happens.

Considering that the callsites of process_in_batches() and genuinely
long-run scripts (see publish-distro.py and process-death-row.py). A
deletion can happen before the batching procedure gets stated or even
while the batches are being processed.

We will select two arbitrary records from the 'input' result set, one
will be deleted before starting the batch processing and the other one
will be deleted while processing the first batch.

    >>> print input.count()
    20

    >>> delete_before_id = input[18].id
    >>> delete_after_id = input[15].id

The 'task' to be performed is adjusted to delete the second probing
record once it's called.

    >>> import transaction
    >>> from canonical.launchpad.interfaces.person import IPersonSet
    >>> cprov = getUtility(IPersonSet).getByName('cprov')

    >>> def nasty_task(binary_publication):
    ...     probe = BinaryPackagePublishingHistory.get(delete_after_id)
    ...     if probe.datesuperseded is None:
    ...          probe.requestDeletion(cprov, 'on-going deletion.')
    ...          transaction.commit()
    ...          print 'gone'
    ...     ignore = binary_publication.binarypackagerelease.name

Deleting the first probing record.

    >>> probe = BinaryPackagePublishingHistory.get(delete_before_id)
    >>> ignore = probe.requestDeletion(cprov, 'deleted before starting')

    >>> transaction.commit()

    >>> print input.count()
    19

process_in_batches() copes just fine with both kind of on-going
deletions in the given result set.

    >>> process_in_batches(input, nasty_task, logger, goal_seconds=1,
    ...                    minimum_chunk_size=10)
    DEBUG: Batch [0..10) [... MiB]
    gone
    DEBUG: Batch [10...) [... MiB]

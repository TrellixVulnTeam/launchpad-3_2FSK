= Processing long iterations in the publishing domain  =

The process_in_batches() function was designed to allow us to perform
read-only python domain iterations over a large result set keeping
memory usage acceptable.

It uses `LoopTuner` to process the given 'task' against batches of
the also given 'input' result set. After each batch is finished it
cleans up the ORM caches, eliminating cached references that won't be
used anymore.

It's important to note that this mechanism is clearly only suitable
for read-only iterations, any modification done in the database
records by 'task' will be ignored.

    >>> from lp.archivepublisher.utils import process_in_batches

    >>> from lp.services.log.logger import FakeLogger
    >>> logger = FakeLogger()

The expected 'input' argument is a `SelectResult`. We will use a
result set containing all PUBLISHED `BinaryPackagePublishingHistory`
records.

    >>> from lp.soyuz.model.publishing import (
    ...     BinaryPackagePublishingHistory)
    >>> from lp.soyuz.enums import (
    ...     PackagePublishingStatus)

    >>> input = BinaryPackagePublishingHistory.selectBy(
    ...     status=PackagePublishingStatus.PUBLISHED, orderBy=['id'])

We will record the number of records to use in later checks

    >>> original_set_size = input.count()

As 'task' we will define a callable that simply extract the binary
publication name.

    >>> def simple_task(binary_publication):
    ...     ignore = binary_publication.binarypackagerelease.name

That's all we need, calling process_in_batches() with the specific
'input', 'task' and a 'logger' instance will run as expected.

It uses the given 'logger' to print information (in debug level) about
the batches being processed and the amount of resident memory used by
the process calling it.

By default is starts with 10000 records batches and the LoopTuner will
adjust it according to the default 'goal_seconds' of 2 s,  when
necessary. The first batch is more than enough to process all
available published records in the sampledata.

    >>> process_in_batches(input, simple_task, logger)
    DEBUG Batch [0...) [... MiB]


== Coping with changes in the given input ==

The current callsites use process_in_batches() mainly to dump
information from all PUBLISHED records in a given context.

Since they are the same parts that publish new packages, we can
guarantee that the result set size will never increase.

On the other hand, it may decrease while the batches get
processed. For instance, when a package gets deleted or obsoleted,
which are always performed by a user, so we can't predict when such
thing will happen.

Considering that the callsites of process_in_batches() and genuinely
long-run scripts (see publish-distro.py and process-death-row.py). A
deletion can happen before the batching procedure gets started or even
while the batches are being processed.

We will select two arbitrary records from the 'input' result set, one
will be deleted before starting the batch processing and the other one
will be deleted while processing the first batch.

    >>> delete_before_id = input[-2].id
    >>> delete_after_id = input[-5].id

The 'task' to be performed is adjusted to delete the second probing
record once it's called.

    >>> import transaction
    >>> from lp.registry.interfaces.person import IPersonSet
    >>> cprov = getUtility(IPersonSet).getByName('cprov')

    >>> def nasty_task(binary_publication):
    ...     probe = BinaryPackagePublishingHistory.get(delete_after_id)
    ...     if probe.datesuperseded is None:
    ...          probe.requestDeletion(cprov, 'on-going deletion.')
    ...          transaction.commit()
    ...          print 'One gone'
    ...     ignore = binary_publication.binarypackagerelease.name

Deleting the first probing record.

    >>> probe = BinaryPackagePublishingHistory.get(delete_before_id)
    >>> ignore = probe.requestDeletion(cprov, 'deleted before starting')

    >>> transaction.commit()

    >>> start_set_size = input.count()
    >>> start_set_size < original_set_size
    True

process_in_batches() copes just fine with both kinds of on-going
deletions in the given result set.

    >>> process_in_batches(input, nasty_task, logger, goal_seconds=1,
    ...                    minimum_chunk_size=10)
    One gone
    DEBUG Batch [0..10) [... MiB]
    DEBUG Batch [10...) [... MiB]

    >>> end_set_size = input.count()
    >>> end_set_size < start_set_size
    True

process_in_batches() also copes with extreme cases, as having all input
vanished between the batches. It might confuse the batch processor,
because it will notice that "more work" has been done than what is
actually available to do.

    >>> def very_nasty_task(binary_publication):
    ...     all_pubs = BinaryPackagePublishingHistory.selectBy(
    ...         status=PackagePublishingStatus.PUBLISHED)
    ...     if all_pubs.count() == 0:
    ...         return
    ...     for pub in all_pubs:
    ...          pub.requestDeletion(cprov, 'on-going deletion.')
    ...     print 'All gone'
    ...     transaction.commit()
    ...     ignore = binary_publication.binarypackagerelease.name

The code deftly handles this situation.

    >>> process_in_batches(input, very_nasty_task, logger, goal_seconds=1,
    ...                    minimum_chunk_size=10)
    All gone
    DEBUG Batch [0..10) [... MiB]

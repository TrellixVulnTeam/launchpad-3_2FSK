
The canonical.database.adapter module provides a Zope database adapter
that can prevent SQL statements from being executed if a request takes
longer than length of time to execute.

Imports and test setup:

    >>> import threading
    >>> import time
    >>> import psycopg
    >>> import transaction
    >>> from zope.component import getUtility
    >>> from zope.app.rdb.interfaces import IZopeDatabaseAdapter
    >>> from sqlos.interfaces import IConnectionName
    >>> from canonical.config import config
    >>> from canonical.launchpad.webapp.adapter import *


The Launchpad database connection is created by getting the
IZopeDatabaseAdapter for the connection name in question.  The
LaunchpadDatabaseAdapter class is registered for the connection name
SQLOS uses:

    >>> name = getUtility(IConnectionName)
    >>> da = getUtility(IZopeDatabaseAdapter, name.name)
    >>> isinstance(da, LaunchpadDatabaseAdapter)
    True


The database adapter is a reconnecting database adapter, so handles
connection drops (as tested elsewhere):

    >>> from canonical.launchpad.webapp.adapter import (
    ...     ReconnectingDatabaseAdapter)
    >>> isinstance(da, ReconnectingDatabaseAdapter)
    True


Statement Timeout
=================

The timeout is set in launchpad.conf file.  By default it is unset,
which corresponds to no timeout:

    >>> print config.launchpad.db_statement_timeout
    None

Connections created with the database adapter will use this timeout as
the Postgres statement timeout (a value of zero means no timeout):

    >>> def current_statement_timeout(cursor):
    ...     cursor.execute('SHOW statement_timeout')
    ...     return cursor.fetchone()[0]
    ...
    >>> set_request_started()
    >>> conn = da()
    >>> cursor = conn.cursor()
    >>> print current_statement_timeout(cursor)
    0
    >>> da.disconnect()
    >>> clear_request_started()


Using the builtin pg_sleep() function, we can trigger the
timeout by sleeping for 200ms with a 100ms statement timeout:

    >>> config.launchpad.db_statement_timeout = 100
    >>> conn = da()
    >>> cursor = conn.cursor()
    >>> set_request_started()
    >>> print current_statement_timeout(cursor)
    100ms
    >>> cursor.execute('SELECT pg_sleep(0.200)')
    Traceback (most recent call last):
      ...
    RequestStatementTimedOut: SELECT pg_sleep(0.200)
    >>> da.disconnect()
    >>> clear_request_started()


The statement_timeout will be adjusted during the transaction, within
a certain precision:

    >>> config.launchpad.db_statement_timeout = 200
    >>> config.launchpad.db_statement_timeout_precision = 100
    >>> conn = da()
    >>> cursor = conn.cursor()
    >>> set_request_started()
    >>> cursor.execute('SELECT pg_sleep(0.050)')
    >>> print current_statement_timeout(cursor)
    200ms
    >>> cursor.execute('SELECT pg_sleep(0.060)')
    >>> cursor.execute('SELECT pg_sleep(0.040)')
    >>> current_statement_timeout(cursor) == '200ms'
    False
    >>> print current_statement_timeout(cursor)
    whatever
    >>> cursor.execute('SELECT pg_sleep(0.050)')
    Traceback (most recent call last):
      ...
    RequestStatementTimedOut: SELECT pg_sleep(0.05)
    >>> da.disconnect()
    >>> clear_request_started()


Set the timeout to 5000ms for the next tests:

    >>> config.launchpad.db_statement_timeout = 5000
    >>> conn = da()
    >>> cursor = conn.cursor()
    >>> set_request_started()
    >>> print current_statement_timeout(cursor)
    5s
    >>> clear_request_started()


Request Timeout
===============

While the postgres statement timeout can help cut short some out of
control requests, it will not help when a request performs a large
number of small requests.

To help with this, the set_request_started() and
clear_request_started() functions are provided as hooks for the web
publisher.  If a request exceeds the timeout, execute() method on
cursors will start raising an exception.

Signal the start of a request:

    >>> set_request_started()

Perform an operation before the time limit expires:

    >>> cursor.execute('SELECT 1')

Once the request has been completed, clear_request_started() should be
called:

    >>> clear_request_started()


The request start time can also be passed to set_request_started().
Set the request start time to 1 minute in the past, and execute
another query:

    >>> set_request_started(time.time() - 60)
    >>> cursor.execute('SELECT 1')
    Traceback (most recent call last):
    ...
    RequestExpired: SELECT 1


When a RequestExpired exception is raised, the current database
transaction will be doomed, and future queries will fail:

    >>> clear_request_started()
    >>> try:
    ...     cursor.execute('SELECT 1')
    ... except psycopg.DatabaseError, e:
    ...     print str(e)
    ERROR:  current transaction is aborted, commands ignored until end of transaction block
    <BLANKLINE>
    SELECT 1

Cleanup:

    >>> conn.rollback()

The LaunchpadDatabaseAdapter assumes that each thread services a
different request, so a request timing out on one thread will not
affect other threads:

    >>> started_request = threading.Event()
    >>> statement_issued = threading.Event()
    >>> def foo():
    ...     set_request_started(time.time() - 60) # timed out
    ...     started_request.set()
    ...     statement_issued.wait()
    ...
    >>> set_request_started()
    >>> thread = threading.Thread(target=foo)
    >>> thread.start()
    >>> started_request.wait()
    >>> cursor.execute('SELECT 1')
    >>> statement_issued.set()
    >>> thread.join()
    >>> clear_request_started()

Similarly, starting a new request in another thread will not reset the
remove the timout:

    >>> started_request = threading.Event()
    >>> statement_issued = threading.Event()
    >>> def foo():
    ...     set_request_started()
    ...     started_request.set()
    ...     statement_issued.wait()
    ...
    >>> set_request_started(time.time() - 60)
    >>> thread = threading.Thread(target=foo)
    >>> thread.start()
    >>> started_request.wait()
    >>> cursor.execute('SELECT 1')
    Traceback (most recent call last):
    ...
    RequestExpired: SELECT 1
    >>> statement_issued.set()
    >>> thread.join()
    >>> clear_request_started()
    >>> conn.rollback()


If no timeout has been set, then requests will not time out:

    >>> config.launchpad.db_statement_timeout = None
    >>> da.disconnect()
    >>> conn = da()
    >>> cursor = conn.cursor()
    >>> set_request_started(time.time() - 60)
    >>> cursor.execute('SELECT 1')
    >>> clear_request_started()


Statement Logging
=================

While a request is in progress, the Launchpad database adapter keeps a
log of the statements executed.  First show that the statement log is
not maintained outside of a request:

    >>> get_request_statements()
    []
    >>> cursor.execute('SELECT 1')
    >>> cursor.execute('SELECT 2')
    >>> get_request_statements()
    []

Now begin a request, and issue a number of statements:

    >>> set_request_started()
    >>> cursor.execute('SELECT 1')
    >>> cursor.execute('SELECT 2')
    >>> for starttime, endtime, statement in get_request_statements():
    ...     print statement
    SELECT 1
    SELECT 2

After we complete the request, the statement log is cleared:

    >>> clear_request_started()
    >>> get_request_statements()
    []


Read Only Transactions
======================

Customizing the database adapter also lets us easily tweak the current
connection. For example we can make it read only:

    >>> cursor.execute("""
    ...     INSERT INTO SourcePackageName(name) VALUES ('fnord')
    ...     """)
    >>> da.readonly()
    >>> cursor.execute("""
    ...     INSERT INTO SourcePackageName(name) VALUES ('fnord2')
    ...     """)
    Traceback (most recent call last):
    ...
    ProgrammingError: ERROR:  transaction is read-only
    ...

The read only status only lasts until the end of the transaction.:

    >>> transaction.abort()
    >>> cursor = da().cursor()
    >>> cursor.execute("""
    ...     INSERT INTO SourcePackageName(name) VALUES ('fnord3')
    ...     """)
    >>> transaction.abort()


Switching Database Users
========================

We can also change the user we are connected as, which is useful in tests.
You should only change the user at the start of a transaction, or else any
uncommitted changes made may be lost:

    >>> da.switchUser(config.statistician.dbuser)
    >>> cursor = da().cursor()
    >>> cursor.execute("SELECT current_user")
    >>> cursor.fetchone()[0] == config.statistician.dbuser
    True
    >>> cursor.execute("""
    ...     INSERT INTO SourcePackageName(name) VALUES ('fnord4')
    ...     """)
    Traceback (most recent call last):
    ...
    ProgrammingError: ERROR:  permission denied for relation sourcepackagename
    ...

Unlike readonly, this is not reset at the end of the transaction:

    >>> transaction.abort()
    >>> cursor = da().cursor()
    >>> cursor.execute("SELECT current_user")
    >>> cursor.fetchone()[0] == config.statistician.dbuser
    True
    >>> cursor.execute("""
    ...     INSERT INTO SourcePackageName(name) VALUES ('fnord4')
    ...     """)
    Traceback (most recent call last):
    ...
    ProgrammingError: ERROR:  permission denied for relation sourcepackagename
    ...
    >>> transaction.abort()

So you need to explicity set the user back:

    >>> da.switchUser()
    >>> cursor = da().cursor()
    >>> cursor.execute("""
    ...     INSERT INTO SourcePackageName(name) VALUES ('fnord4')
    ...     """)
    >>> cursor.execute("SELECT current_user")
    >>> cursor.fetchone()[0] == config.launchpad.dbuser
    True


Deadlocks & Serialization errors
================================

The Psycopg Database Adapter will catch serialization, deadlock
and integrity exceptions and reraise them as Retry exceptions. A
serialization exception occurs when a connection is running in the
SERIALIZABLE transaction isolation level and PostgreSQL is detects
that your connections reads are now inconsistant due to a concurrent
update. Retrying the transaction is the documented way of handling this
case. A deadlock occurs when two connections are waiting on locks both
held by the other connection. The best way of handling this for a web
application seems also to be to retry the transaction.

This is not being tested upstream in the SVN PsycopgDA branch, as testing
requires access to a PostgreSQL database and I can't write a suitably
generic test harness at this time. So we test here in Launchpad to catch
regressions, which I expect will occur with PostgreSQL and psycopg version
changes. Also, the Retry exception being raised on IntegrityViolations is
a Launchpad specific hack - I am unsure if all users of psycopgda would
want this functionality.

In order to catch the psycopg connections, Psycopgda needs to return
wrapped ZopeConnection and ZopeCursor instances (the wrapped connection
is only needed to make the cursor() method return a wrapped cursor).

    >>> from psycopgda.adapter import PsycopgConnection, PsycopgCursor
    >>> con = da()
    >>> cur = con.cursor()
    >>> isinstance(con, PsycopgConnection)
    True
    >>> isinstance(cur, PsycopgCursor)
    True

Test integrity violations. This one is easy. Note that the exception
raised is a subclass of both IntegrityError and Retry, to allow the
publishing machinery to handle the Retry exception without modification
and to allow existing code that handles IntegrityErrors themselves to
continue functioning as normal.

    >>> from zope.publisher.interfaces import Retry
    >>> cur.execute("UPDATE JabberID SET jabberid='dupe' WHERE id=1")
    >>> try:
    ...     cur.execute("UPDATE JabberID SET jabberid='dupe' WHERE id=2")
    ... except Retry, error:
    ...     print '%s raised' % error.__class__
    ...     print str(error)
    canonical.launchpad.webapp.adapter.RetryPsycopgIntegrityError raised
    ERROR:  duplicate key violates unique constraint "jabberid_jabberid_key"
    <BLANKLINE>
    UPDATE JabberID SET jabberid='dupe' WHERE id=2

Early versions of psycopg would raise some integrity violations as
ProgrammingErrors. We should confirm we are running a version of psycopg
that does not have this problem.

    >>> con.rollback()
    >>> cur = con.cursor()
    >>> try:
    ...     cur.execute("""
    ...         INSERT INTO JabberID (person, jabberid)
    ...         VALUES (1, 'markshuttleworth@jabber.org')
    ...         """)
    ... except Retry, error:
    ...     print '%s raised' % error.__class__
    canonical.launchpad.webapp.adapter.RetryPsycopgIntegrityError raised


To trigger a serialization exception using the database adapter, we need
to use two threads that are synchronized to ensure the querys occur in the
correct order.

    >>> import traceback
    >>> from threading import Thread, Event
    >>> steps = [Event(), Event()]
    >>> class STask1(Thread):
    ...     def run(self):
    ...         try:
    ...             self.traceback = None
    ...             con = da()
    ...             cur = con.cursor()
    ...             cur.execute("SELECT name FROM Person WHERE id=22")
    ...             self.traceback = repr(list(cur.fetchall()))
    ...             steps[0].set()
    ...             steps[1].wait()
    ...             cur.execute("UPDATE Person SET name='argh' WHERE id=22")
    ...             transaction.commit()
    ...             self.traceback = None
    ...         except:
    ...             self.traceback = traceback.format_exc()
    ...             steps[0].set()
    ...         con.close()

    >>> class STask2(threading.Thread):
    ...     def run(self):
    ...         try:
    ...             self.traceback = None
    ...             con = da()
    ...             cur = con.cursor()
    ...             steps[0].wait()
    ...             cur.execute("UPDATE Person SET name='whatever' WHERE id=22")
    ...             transaction.commit()
    ...         except Exception:
    ...             self.traceback = traceback.format_exc()
    ...         steps[1].set()
    ...         con.close()

    >>> task1 = STask1()
    >>> task2 = STask2()
    >>> task1.start()
    >>> task2.start()
    >>> task1.join()
    >>> task2.join()

    Only one thread raised an exception, and that exception is a
    Retry exception.

    >>> bool(task1.traceback and task2.traceback)
    False
    >>> print task1.traceback or task2.traceback
    Traceback (most recent call last):
    ...
    Retry: ...


Deadlocks can be tested for the same way as serialization exceptions.

    >>> steps = [Event(), Event()]
    >>> class DTask1(Thread):
    ...     def run(self):
    ...         try:
    ...             self.traceback = None
    ...             con = da()
    ...             cur = con.cursor()
    ...             cur.execute("LOCK Person IN ACCESS EXCLUSIVE MODE")
    ...             steps[0].set()
    ...             steps[1].wait()
    ...             cur.execute("LOCK Bug IN ACCESS EXCLUSIVE MODE")
    ...         except:
    ...             self.traceback = traceback.format_exc()
    ...             steps[0].set()

    >>> class DTask2(threading.Thread):
    ...     def run(self):
    ...         try:
    ...             self.traceback = None
    ...             con = da()
    ...             cur = con.cursor()
    ...             steps[0].wait()
    ...             cur.execute("LOCK Bug IN ACCESS EXCLUSIVE MODE")
    ...             steps[1].set()
    ...             cur.execute("LOCK Person IN ACCESS EXCLUSIVE MODE")
    ...         except Exception:
    ...             self.traceback = traceback.format_exc()
    ...             steps[1].set()

    >>> task1 = DTask1()
    >>> task2 = DTask2()
    >>> task1.start()
    >>> task2.start()
    >>> task1.join()
    >>> task2.join()

Only one of the threads will have raised a Retry exception; the other
thread will have succeeded.

    >>> bool(task1.traceback and task2.traceback)
    False
    >>> print task1.traceback or task2.traceback
    Traceback (most recent call last):
    ...
    Retry: ...
